[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Homework 3",
    "section": "",
    "text": "Jaryt Salvo\nDate: 9/30/24\nFall 2024 | MATH6410 Probability Theory\n\nThis project contains solutions to Homework 3 from the Probability Theory course (Math 6410) using Clojure. The code provided here demonstrates a functional programming approach to solving various probability problems, including discrete and continuous probability distributions, conditional probability, and applications to real-world scenarios.\nThe code is organized into different sections corresponding to each homework problem, with detailed explanations of the logic and mathematical steps involved. By using Clojure and its associated libraries, such as scicloj.clay for rendering, fastmath for mathematical operations, and custom utility functions, we provide a comprehensive solution set for the assigned problems.\n\nProblem Overview\n\nSigma Algebras: Proving properties of set collections.\nSubset Counting: Proving the number of subsets for a set with n elements.\nOccupancy Problem: Calculating probabilities for balls in cells.\nCoin Flipping Game: Analyzing probabilities in a sequential game.\nConditional Probability: Rodent litter problem using Bayes’ theorem.\nProbability Density Functions: Proving properties of a defined function.\nAI Error Distribution: Applying Stirling numbers to a practical scenario.\nCovid Symptom Spread: Modeling disease spread in a waiting room.\n\n\n\nUtils\nThe utils.clj file contains various utility functions and helpers used throughout the homework solutions. It includes:\n\nFormatting functions for markdown and LaTeX rendering\nStatistical functions for probability distributions\nHelper functions for calculations and data manipulations\n\nThese utilities are designed to streamline the problem-solving process and provide reusable components for statistical computations.\nThe code in the src/assignments folder was rendered with Clay and deployed with Github Pages.\n\nsource: src/index.clj",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Jaryt Salvo</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.utils.html",
    "href": "assignments.hw3.utils.html",
    "title": "Utils",
    "section": "",
    "text": "(ns assignments.hw3.utils\n  (:require\n   [clojure.math.combinatorics :as combo]\n   [scicloj.kindly.v4.api :as kindly]\n   [scicloj.kindly.v4.kind :as kind]\n   (uncomplicate.neanderthal\n    [core :refer [mv]]\n    [linalg :refer [trf! tri!]]\n    [native :refer [dge dv]])))\n\n\n(kind/md \"## Utils\")\n\nFunctions from HW3\nQ2: Subset counting\n\n(defn count-subsets [n]\n  (Math/pow 2 n))\n\nQ3: Occupancy problem\n\n(defn probability-one-cell-empty [n]\n  (/ (* n (Math/pow (dec n) (dec n)))\n     (Math/pow n n)))\n\nQ4: Coin flipping game\n\n(defn simulate-game []\n  (loop [player :A]\n    (if (= (rand-int 2) 0)                          ; 0 represents heads\n      player\n      (recur (if (= player :A) :B :A)))))\n\n\n(defn simulate-many-games [n]\n  (let [results (repeatedly n simulate-game)\n        a-wins (count (filter #(= % :A) results))]\n    (double (/ a-wins n))))\n\nQ7: AI Error Distribution\n\n(defn factorial [n]\n  (apply * (range 1 (inc n))))\n\n\n(defn stirling2 [n k]\n  (cond\n    (or (= k 0) (&gt; k n)) 0\n    (or (= k 1) (= k n)) 1\n    :else (+ (* k (stirling2 (dec n) k))\n             (stirling2 (dec n) (dec k)))))\n\n\n(defn probability-all-circuits-receive-error []\n  (let [n 10                                            ; number of errors\n        k 8                                             ; number of circuits\n        total-ways (Math/pow k n)\n        favorable-ways (* (factorial k) (stirling2 n k))]\n    (/ favorable-ways total-ways)))\n\n\n(defn simulate-error-distribution []\n  (let [circuits (vec (repeat 8 0))]\n    (reduce (fn [acc _]\n              (update acc (rand-int 8) inc))\n            circuits\n            (range 10))))\n\n\n(defn all-circuits-have-error? [distribution]\n  (every? pos? distribution))\n\n\n(defn monte-carlo-simulation [num-simulations]\n  (let [successes (atom 0)]\n    (doseq [_ (range num-simulations)]\n      (when (all-circuits-have-error? (simulate-error-distribution))\n        (swap! successes inc)))\n    (double (/ @successes num-simulations))))\n\nHelper functions\n\n(defn joint-probability\n  \"Helper function to calculate the joint probability of a subset of events.\"\n  [probs subset]\n  (reduce * (map #(nth probs %) subset)))\n\n\n(defn subsets\n  \"Generate all non-empty subsets of a set of indices.\"\n  [s]\n  (filter seq (combo/subsets s)))\n\n\n(defn power-set\n  \"Returns the power set of a given collection (set or vector).\"\n  [coll]\n  (set (map set (combo/subsets (seq coll)))))\n\n\n(defn probability-at-least-one\n  \"Calculate the probability of at least one event occurring given a collection of probabilities.\n       Uses the inclusion-exclusion principle for any number of events.\"\n  [probs]\n  (let [p-none (reduce * (map #(- 1 %) probs))]\n    (- 1 p-none)))\n\n\n(defn solve-probabilities [b1 b2]\n  (let [A (dge 2 2 [1 1\n                    3 -1]\n               {:layout :row})\n        b (dv [b1 b2])\n        LU (trf! A)\n        x (mv (tri! LU) b)]\n    x))\n\n\n(defn nck\n  \"Calculates the binomial coefficient (n choose k) using iterative and recursive method.\"\n  [n k]\n  (if (or (&lt; k 0) (&gt; k n))\n    0\n    (let [k (min k (- n k))]\n      (loop [result 1N, i 0]\n        (if (= i k)\n          result\n          (recur\n           (/ (* result (- n i)) (inc i))\n           (inc i)))))))\n\n\nsource: src/assignments/hw3/utils.clj",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Utils</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q1.html",
    "href": "assignments.hw3.q1.html",
    "title": "Question 1",
    "section": "",
    "text": "(ns assignments.hw3.q1\n  (:require [assignments.hw3.utils :refer :all]))\n\n\n\n1) Let S be a sample space.\na) Show that the collection B = {∅, S} is a sigma algebra.\nTo show that \\(B = {∅, S}\\) is a sigma algebra, we need to verify three properties:\n\n\\(∅ ∈ B\\) (The empty set is in B)\nIf \\(A ∈ B\\), then \\(A^c ∈ B\\) (B is closed under complementation)\nIf \\(A1, A2, ... ∈ B\\), then \\(∪Ai ∈ B\\) (B is closed under countable unions)\n\nProof:\n\n\\(∅ ∈ B\\): This is true by definition of B.\nClosure under complementation:\n\n\\(∅^c = S\\), which is in B\n\\(S^c = ∅\\), which is in B\n\nClosure under countable unions:\n\n\\(∅ ∪ ∅ = ∅\\), which is in B\n\\(∅ ∪ S = S\\), which is in B\n\\(S ∪ S = S\\), which is in B\n\n\nTherefore, \\(B = {∅, S}\\) is a sigma algebra.\nb) Let B = { all subsets of S, including S itself }. Show that B is a sigma algebra.\nTo show that B = { all subsets of S, including S itself } is a sigma algebra, we need to verify the same three properties:\n\n\\(∅ ∈ B\\) (The empty set is in B)\nIf \\(A ∈ B\\), then \\(A^c ∈ B\\) (B is closed under complementation)\nIf \\(A1, A2, ... ∈ B\\), then \\(∪Ai ∈ B\\) (B is closed under countable unions)\n\nProof:\n\n\\(∅ ∈ B\\): This is true because the empty set is a subset of S.\nClosure under complementation: For any \\(A ∈ B\\), \\(A^c\\) is also a subset of S, so \\(A^c ∈ B\\).\nClosure under countable unions: For any countable collection of subsets \\(A1, A2, ... ∈ B\\), their union \\(∪Ai\\) is also a subset of S, so \\(∪Ai ∈ B\\).\n\n\n(answer \"Therefore, B = { all subsets of S, including S itself } is a sigma algebra.\")\n\n\nTherefore, B = { all subsets of S, including S itself } is a sigma algebra.\n\nc) Show that the intersection of two sigma algebras is a sigma algebra.\nLet B1 and B2 be two sigma algebras on a sample space S. We need to show that B = B1 ∩ B2 is also a sigma algebra.\nProof:\n\n\\(∅ ∈ B\\): Since \\(B1\\) and \\(B2\\) are sigma algebras, \\(∅ ∈ B1\\) and \\(∅ ∈ B2\\). Therefore, \\(∅ ∈ B1 ∩ B2 = B\\).\nClosure under complementation: Let \\(A ∈ B\\). This means \\(A ∈ B1\\) and \\(A ∈ B2\\). Since \\(B1\\) and \\(B2\\) are sigma algebras, \\(A^c ∈ B1\\) and \\(A^c ∈ B2\\). Therefore, \\(A^c ∈ B1 ∩ B2 = B\\).\nClosure under countable unions: Let \\(A1, A2, ... ∈ B\\). This means for each \\(i\\), \\(Ai ∈ B1\\) and \\(Ai ∈ B2\\).\nSince \\(B1\\) and \\(B2\\) are sigma algebras, \\(∪Ai ∈ B1\\) and \\(∪Ai ∈ B2\\). Therefore, \\(∪Ai ∈ B1 ∩ B2 = B\\).\n\n\n(answer \"Therefore, the intersection of two sigma algebras is a sigma algebra.\")\n\n\nTherefore, the intersection of two sigma algebras is a sigma algebra.\n\nConclusion:\nWe have shown that:\n\nThe collection \\(B = {∅, S}\\) is a sigma algebra.\nThe collection of all subsets of \\(S\\) is a sigma algebra.\nThe intersection of two sigma algebras is a sigma algebra.\n\nThese results are fundamental in measure theory and probability theory, providing the basis for defining probability measures on sets.\n\nsource: src/assignments/hw3/q1.clj",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Question 1</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q2.html",
    "href": "assignments.hw3.q2.html",
    "title": "Question 2",
    "section": "",
    "text": "(ns assignments.hw3.q2\n  (:require\n   [assignments.hw3.utils :refer :all]))\n\n\n\n2) Suppose that a sample space S has n elements. Prove that the number of subsets that can be formed from the elements of S is \\(2^n\\).\nTo prove that the number of subsets that can be formed from the elements of S is \\(2^n\\), we can use the following reasoning:\n\nCounting the Empty Subset: There is exactly one empty subset, ∅, which contains no elements.\nCounting Subsets with One Element: For each element in S, we can form a subset containing just that element. There are n such subsets.\nCounting Subsets with Two Elements: We can choose any 2 elements from n elements to form a subset. The number of such subsets is \\(\\binom{n}{2}\\).\nContinuing the Pattern: We can form subsets with 3 elements (\\(\\binom{n}{3}\\) subsets), 4 elements (\\(\\binom{n}{4}\\) subsets), and so on, up to n elements (1 subset containing all elements of S).\nSum of All Possibilities: The total number of subsets is the sum of all these possibilities:\n\n\\[1 + \\binom{n}{1} + \\binom{n}{2} + \\binom{n}{3} + ... + \\binom{n}{n-1} + \\binom{n}{n}\\]\n\nBinomial Theorem: This sum is equal to \\(2^n\\) according to the binomial theorem:\n\n\\[(1 + 1)^n = \\sum_{k=0}^n \\binom{n}{k} = 2^n\\]\n\nAlternative Proof Using Binary Representation: We can also prove this using a binary representation approach:\n\nFor each element in S, we have two choices: include it in the subset (1) or not (0).\nThis gives us a binary string of length n for each subset.\nThe number of possible binary strings of length n is \\(2^n\\).\n\nConclusion: Therefore, the number of subsets that can be formed from a sample space S with n elements is \\(2^n\\).\n\n\n(answer \"The number of subsets that can be formed from a sample space S with n elements is $2^n$.\")\n\n\nThe number of subsets that can be formed from a sample space S with n elements is \\(2^n\\).\n\nVerification:\n\n(comment\n  (defn count-subsets [n]\n    (Math/pow 2 n)))\n\nLet’s verify this for a few values of n:\n\n(doseq [n (range 1 6)]\n  (md (str \"For n = \" n \", number of subsets = \" (count-subsets n))))\n\n\nnil\n\n\n(def subset-counts\n  (map (fn [n] {:n n :subsets (count-subsets n)}) (range 1 6)))\n\n\n(into [] subset-counts)\n\n\n[{:n 1, :subsets 2.0}\n {:n 2, :subsets 4.0}\n {:n 3, :subsets 8.0}\n {:n 4, :subsets 16.0}\n {:n 5, :subsets 32.0}]\n\nInterpretation:\n\nThe result \\(2^n\\) represents the power set of S, which is the set of all subsets of S, including the empty set and S itself.\nThis formula is crucial in probability theory, as it helps us count the number of possible outcomes in various scenarios.\nIn set theory, this result is fundamental to understanding the relationship between a set and its subsets.\n\nConclusion:\nWe have proven that for a sample space S with n elements, the number of subsets is \\(2^n\\). This result is a cornerstone in set theory, combinatorics, and probability theory, with wide-ranging applications in mathematics and computer science.\n\nsource: src/assignments/hw3/q2.clj",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Question 2</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q3.html",
    "href": "assignments.hw3.q3.html",
    "title": "Question 3",
    "section": "",
    "text": "(ns assignments.hw3.q3\n  (:require\n   [assignments.hw3.utils :refer :all]\n   [fastmath.core :as m]\n   [scicloj.hanamicloth.v1.api :as haclo]\n   [tablecloth.api :as tc]))\n\n\n\n3) If \\(n\\) balls are placed at random into \\(n\\) cells, find the probability that exactly one cell remains empty.\nSolution:\nTo solve this problem, we’ll use the following approach:\n\nCalculate the total number of ways to place n balls into n cells.\nCalculate the number of ways to place n balls into n cells with exactly one cell empty.\nDivide the result from step 2 by the result from step 1 to get the probability.\n\nStep 1: Total number of ways\nTotal ways = \\(n^n\\)\nStep 2: Number of ways with exactly one cell empty\nWe can calculate this using the following steps:\n\nChoose 1 cell to be empty: \\(\\binom{n}{1} = n\\) ways\nDistribute \\(n-1\\) balls among \\(n-1\\) cells: \\((n-1)^{n-1}\\) ways\nMultiply these together: \\(n \\cdot (n-1)^{n-1}\\) ways\n\nStep 3: Calculate the probability\nP(exactly one cell empty) = \\(\\frac{n \\cdot (n-1)^{n-1}}{n^n}\\)\n\n(comment\n  (defn probability-one-cell-empty [n]\n    (/ (* n (m/pow (dec n) (dec n)))\n       (m/pow n n))))\n\nLet’s calculate this probability for a few values of n:\n\n(def probabilities\n  (map (fn [n] {:n n :probability (probability-one-cell-empty n)})\n       (range 2 11)))\n\n\n(into [] probabilities)\n\n\n[{:n 2, :probability 0.5}\n {:n 3, :probability 0.4444444444444444}\n {:n 4, :probability 0.421875}\n {:n 5, :probability 0.4096}\n {:n 6, :probability 0.4018775720164609}\n {:n 7, :probability 0.39656945660396603}\n {:n 8, :probability 0.39269590377807617}\n {:n 9, :probability 0.38974434312894585}\n {:n 10, :probability 0.387420489}]\n\nInterpretation:\n\nAs \\(n\\) increases, the probability of having exactly one cell empty decreases.\nThis makes sense intuitively: with more balls and cells, it becomes less likely to have exactly one cell empty.\nFor large \\(n\\), this probability approaches \\(\\frac{1}{e} \\approx 0.3679\\), which is a well-known result in probability theory.\n\nVerification:\nWe can verify that our formula approaches \\(\\frac{1}{e}\\) for large n:\n\n(let [vals (range 1 50)\n      probs (map probability-one-cell-empty vals)\n      data (tc/dataset {:ns    vals\n                        :probs probs})]\n  (-&gt; data\n      (haclo/layer-line\n       {:=x :ns :=x-title \"n\" :=mark-size 7\n        :=y :probs :=y-title \"P(exactly one cell empty)\"})\n      (haclo/update-data (fn [_]\n                           (tc/dataset\n                            {:ns    (range 1 50)\n                             :e-inv (repeat 50 (/ 1 Math/E))})))\n      (haclo/layer-line\n       {:=x :ns :=y :e-inv :=mark-color :red})))\n\n\n\n(let [large-n 143 ;last value before NaN--n^n overflows\n      prob (probability-one-cell-empty large-n)\n      e-inverse (/ 1 Math/E)]\n  (answer (str \"For n = \" large-n \": \\t\\t\n                Calculated probability: \" (format \"%.5f\" prob) \"\\t\\t\n                $1/e$: \" (format \"%.5f\" e-inverse))))\n\n\nFor n = 143:\nCalculated probability: 0.36917\n\\(1/e\\): 0.36788\n\nConclusion:\nWe have successfully derived and implemented a formula for the probability of exactly one cell remaining empty when n balls are placed randomly into n cells. This problem is related to the concept of ‘occupancy problems’ in probability theory and has applications in various fields, including computer science (e.g., hash table collisions) and biology (e.g., certain population models).\n\nsource: src/assignments/hw3/q3.clj",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Question 3</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q4.html",
    "href": "assignments.hw3.q4.html",
    "title": "Question 4",
    "section": "",
    "text": "(ns assignments.hw3.q4\n   (:require\n    [assignments.hw3.utils :refer :all]\n    [tablecloth.api :as tc]\n    [scicloj.hanamicloth.v1.api :as haclo]))\n\n\n\n4) Two players, A and B, alternately and independently flip a coin and the first player to obtain a head wins. Assume player A flips first.\na) If the coin is fair, what is the probability that A wins?\nTo solve this problem, let’s consider the possible scenarios:\n\nA wins on the first flip (probability 1/2)\nIf A doesn’t win on the first flip, B flips. If B doesn’t win, we’re back to A’s turn, but now in the same situation as at the start of the game.\n\nLet p be the probability that A wins. We can write an equation:\n\\[p = \\frac{1}{2} + \\frac{1}{2} \\cdot \\frac{1}{2} \\cdot p\\]\nThis equation states that A wins either on the first flip (1/2) or if both A and B get tails (1/2 * 1/2), bringing us back to the original situation (p).\nSolving this equation:\n\\[p = \\frac{1}{2} + \\frac{1}{4}p\\]\n\\[\\frac{3}{4}p = \\frac{1}{2}\\]\n\\[p = \\frac{2}{3}\\]\n\n(def probability-a-wins\n   (fn [p]\n     (/ p (- 1 (* (- 1 p) (- 1 p))))))\n\n\n(def probability-a-wins-fair\n   (probability-a-wins 0.5))\n\n\n(answer\n  (str \"The probability that A wins with a fair coin is \"\n       (format \"%.5f\" probability-a-wins-fair)))\n\n\nThe probability that A wins with a fair coin is 0.66667\n\nWe can verify this result by simulating many games:\n\n(comment\n   (defn simulate-game []\n     (loop [player :A]\n       (if (= (rand-int 2) 0)                          ; 0 represents heads\n         player\n         (recur (if (= player :A) :B :A))))))\n\n\n(comment\n   (defn simulate-many-games [n]\n     (let [results (repeatedly n simulate-game)\n           a-wins (count (filter #(= % :A) results))]\n       (double (/ a-wins n)))))\n\n\n(let [num-simulations 1000000\n       simulated-prob (simulate-many-games num-simulations)]\n   (answer\n    (str \"Simulated probability after \" num-simulations \" games: \" (format \"%.4f\" simulated-prob))))\n\n\nSimulated probability after 1000000 games: 0.6673\n\nb) Suppose that P(head) = p, not necessarily 1/2. What is the probability that A wins?\nLet’s use the same approach as in part (a), but now with a general probability p for heads.\nLet q be the probability that A wins. We can write an equation:\n\\[q = p + (1-p)(1-p)q\\]\nThis equation states that A wins either on the first flip (p) or if both A and B get tails ((1-p)(1-p)), bringing us back to the original situation (q).\nSolving this equation:\n\\[q = p + (1-p)^2q\\]\n\\[q - (1-p)^2q = p\\]\n\\[q(1 - (1-p)^2) = p\\]\n\\[q = \\frac{p}{1 - (1-p)^2}\\]\n\\[q = \\frac{p}{2p - p^2}\\]\n\\[q = \\frac{1}{2 - p}\\]\nLet’s calculate this probability for a few values of p:\n\n(def probabilities\n   (map (fn [p] {:p p :probability (probability-a-wins p)})\n        [0.1 0.3 0.5 0.7 0.9]))\n\n\n(into [] probabilities)\n\n\n[{:p 0.1, :probability 0.5263157894736844}\n {:p 0.3, :probability 0.5882352941176471}\n {:p 0.5, :probability 0.6666666666666666}\n {:p 0.7, :probability 0.7692307692307693}\n {:p 0.9, :probability 0.9090909090909092}]\n\nInterpretation:\n\nWhen p = 0.5 (fair coin), we get the same result as in part (a): 2/3.\nAs p increases, A’s probability of winning increases, which makes sense as A gets the first chance to flip.\nAs p approaches 1, A’s probability of winning approaches 1, as A would almost certainly win on the first flip.\nAs p approaches 0, A’s probability of winning approaches 1/2, as the game would likely go on for many flips, slightly favoring A who goes first.\n\n\n(answer \"The probability that A wins when P(head) = p is 1 / (2 - p)\")\n\n\nThe probability that A wins when P(head) = p is 1 / (2 - p)\n\nSimulation Verification for Part b:\nLet’s create a simulation to verify our formula for different values of p:\n\n(defn simulate-game-b [p]\n   (loop [player :A]\n     (if (&lt; (rand) p)\n       player\n       (recur (if (= player :A) :B :A)))))\n\n\n(defn simulate-many-games-b [n p]\n   (let [results (repeatedly n #(simulate-game-b p))\n         a-wins (count (filter #(= % :A) results))]\n     (double (/ a-wins n))))\n\n\n(def simulation-results\n   (for [p [0.1 0.3 0.5 0.7 0.9]]\n     (let [theoretical (probability-a-wins p)\n           simulated (simulate-many-games-b 1000000 p)]\n       {:p p\n        :theoretical theoretical\n        :simulated simulated\n        :difference (- theoretical simulated)})))\n\n\n(answer \"Simulation results comparing theoretical and simulated probabilities:\")\n\n\nSimulation results comparing theoretical and simulated probabilities:\n\n\n(into [] simulation-results)\n\n\n[{:p 0.1,\n  :theoretical 0.5263157894736844,\n  :simulated 0.526619,\n  :difference -3.032105263155449E-4}\n {:p 0.3,\n  :theoretical 0.5882352941176471,\n  :simulated 0.587536,\n  :difference 6.992941176471312E-4}\n {:p 0.5,\n  :theoretical 0.6666666666666666,\n  :simulated 0.666558,\n  :difference 1.0866666666664582E-4}\n {:p 0.7,\n  :theoretical 0.7692307692307693,\n  :simulated 0.76968,\n  :difference -4.4923076923075733E-4}\n {:p 0.9,\n  :theoretical 0.9090909090909092,\n  :simulated 0.909293,\n  :difference -2.0209090909084626E-4}]\n\nConclusion:\nWe have derived, implemented, and verified formulas for the probability of player A winning in a coin-flipping game against player B, both for a fair coin and for a coin with an arbitrary probability of heads. The simulation results closely match our theoretical calculations, confirming the accuracy of our formula.\nThis problem demonstrates the application of probability in game theory and illustrates how initial advantages (going first) can affect the overall probability of winning in sequential games. It also showcases how simulations can be used to verify theoretical results in probability problems.\nAdd the following code for plotting\n\n(def simulation-data\n   (tc/dataset\n    (concat\n     (map #(assoc % :type \"Theoretical\" :probability (:theoretical %)) simulation-results)\n     (map #(assoc % :type \"Simulated\" :probability (:simulated %)) simulation-results))))\n\n\n(-&gt; simulation-data\n     (haclo/layer-line\n      {:=x :p :=x-title \"P(Head)\" :=title \"Probability of A Winning\"\n       :=y :probability :=y-title \"Probability of A Winning\"\n       :=color :type})\n     (haclo/layer-point\n      {:=x :p       :=y :probability :=color :type}))\n\n\nThe plot above shows the theoretical probability (line) and simulated results (points) for different values of P(Head). As we can see, the simulated results closely follow the theoretical curve, confirming the accuracy of our formula and simulation.\n\n(def difference-data\n  (tc/dataset simulation-results))\n\n\n(-&gt; difference-data\n     (haclo/layer-line\n      {:=x :p :=x-title \"P(Head)\" :=title \"Difference between Theoretical and Simulated Probabilities\"\n       :=y :difference :=y-title \"Theoretical - Simulated\"}))\n\n\nThis second plot shows the difference between the theoretical and simulated probabilities. The closer the line is to zero, the better the simulation matches the theoretical prediction. As we can see, the differences are very small, typically within ±0.005, which indicates a good agreement between theory and simulation.\n\nsource: src/assignments/hw3/q4.clj",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Question 4</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q5.html",
    "href": "assignments.hw3.q5.html",
    "title": "Question 5",
    "section": "",
    "text": "(ns assignments.hw3.q5\n  (:require\n    [assignments.hw3.utils :refer :all]))\n\n\n\n5) Two litters of a particular rodent species have been born, one with two brown-haired and one gray-haired (litter 1), and the other with three brown-haired and two gray-haired (litter 2). We select a litter at random and then select an offspring at random from the selected litter.\na) What is the probability that the animal chosen is brown-haired?\nTo solve this problem, let’s use the law of total probability. We’ll calculate the probability of choosing a brown-haired animal from each litter and then combine these probabilities.\nLet’s define our events: - L1: Litter 1 is chosen - L2: Litter 2 is chosen - B: A brown-haired animal is chosen\nWe know: - \\(P(L1) = P(L2) = 1/2\\) (equal chance of choosing either litter) - \\(P(B|L1) = 2/3\\) (2 out of 3 animals in litter 1 are brown) - \\(P(B|L2) = 3/5\\) (3 out of 5 animals in litter 2 are brown)\n\\[P(B) = P(B|L1) * P(L1) + P(B|L2) * P(L2)\\]\n\\[P(B) = \\frac{2}{3} * \\frac{1}{2} + \\frac{3}{5} * \\frac{1}{2}\\]\n\\[P(B) = \\frac{1}{3} + \\frac{3}{10}\\]\n\\[P(B) = \\frac{10}{30} + \\frac{9}{30}\\]\n\\[P(B) = \\frac{19}{30}\\]\n\n(defn probability-brown []\n      (/ 19 30))\n\n\n(answer (str \"The probability that the chosen animal is brown-haired is \" (probability-brown) \" or approximately \" (format \"%.4f\" (double (probability-brown)))))\n\n\nThe probability that the chosen animal is brown-haired is 19/30 or approximately 0.6333\n\nb) Given that a brown-haired offspring was selected, what is the probability that the sampling was from litter 1?\nTo solve this, we’ll use Bayes’ theorem. We want to find P(L1|B).\nBayes’ theorem states: \\(P(L1|B) = \\frac{P(B|L1) * P(L1)}{P(B)}\\)\nWe already know: - \\(P(B|L1) = \\frac{2}{3}\\) - \\(P(L1) = \\frac{1}{2}\\) - \\(P(B) = \\frac{19}{30}\\) (from part a)\n\\[P(L1|B) = \\frac{P(B|L1) * P(L1)}{P(B)}\\]\n\\[P(L1|B) = \\frac{\\frac{2}{3} * \\frac{1}{2}}{\\frac{19}{30}}\\]\n\\[P(L1|B) = \\frac{\\frac{1}{3}}{\\frac{19}{30}}\\]\n\\[P(L1|B) = \\frac{10}{19}\\]\n\n(defn probability-litter1-given-brown []\n      (/ 10 19))\n\n\n(answer (str \"Given that a brown-haired offspring was selected, the probability that the sampling was from litter 1 is \"\n             (probability-litter1-given-brown)\n             \" or approximately \"\n             (format \"%.4f\" (double (probability-litter1-given-brown)))))\n\n\nGiven that a brown-haired offspring was selected, the probability that the sampling was from litter 1 is 10/19 or approximately 0.5263\n\nInterpretation:\n\nThe probability of choosing a brown-haired animal (19/30 ≈ 0.6333) is slightly higher than 0.5, which makes sense because both litters have more brown-haired animals than gray-haired ones.\nGiven that a brown-haired animal was chosen, the probability that it came from litter 1 (10/19 ≈ 0.5263) is slightly higher than 0.5. This is because:\n\nLitter 1 has a higher proportion of brown-haired animals (2/3) compared to litter 2 (3/5).\nHowever, litter 2 has more brown-haired animals in total (3 vs 2).\nThese factors nearly balance out, resulting in a probability just slightly favoring litter 1.\n\n\nConclusion:\nThis problem demonstrates the application of the law of total probability and Bayes’ theorem in a real-world scenario. It shows how we can calculate probabilities when dealing with multiple groups (litters) and how we can update our probabilities based on new information (knowing the chosen animal is brown-haired). These concepts are fundamental in probability theory and have wide-ranging applications in fields such as genetics, medical diagnosis, and data analysis.\n\nsource: src/assignments/hw3/q5.clj",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Question 5</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q6.html",
    "href": "assignments.hw3.q6.html",
    "title": "Question 6",
    "section": "",
    "text": "(ns assignments.hw3.q6\n  (:require\n    [assignments.hw3.utils :refer :all]))\n\n\n\n6) Let X be a continuous random variable with pdf \\(f(x)\\) and cdf \\(F(x)\\). For a fixed number \\(x_0\\), define the function\n\\[g(x) = \\begin{cases}\n    \\frac{f(x)}{1 - F(x_0)} & \\text{if } x \\geq x_0 \\\\\n    0 & \\text{if } x &lt; x_0\n\\end{cases}\\]\nProve that \\(g(x)\\) is a pdf (Assume that \\(F(x_0) &lt; 1\\).)\nTo prove that \\(g(x)\\) is a probability density function (pdf), we need to show two properties:\n\n\\(g(x) \\geq 0\\) for all \\(x\\)\nThe integral of \\(g(x)\\) over its entire domain equals 1\n\nLet’s prove each of these properties:\n1. Non-negativity: \\(g(x) \\geq 0\\) for all \\(x\\)\nFor \\(x &lt; x_0\\), \\(g(x) = 0\\), which satisfies the non-negativity condition.\nFor \\(x \\geq x_0\\): - \\(f(x) \\geq 0\\) (since \\(f(x)\\) is a pdf) - \\(1 - F(x_0) &gt; 0\\) (since we’re given that \\(F(x_0) &lt; 1\\))\nTherefore, \\(g(x) = f(x) / (1 - F(x_0)) \\geq 0\\) for all \\(x \\geq x_0\\).\nThus, \\(g(x) \\geq 0\\) for all \\(x\\).\n2. Integral of \\(g(x)\\) equals 1\nLet’s calculate the integral of \\(g(x)\\) over its entire domain:\n\\[\\int_{-\\infty}^{\\infty} g(x) dx = \\int_{-\\infty}^{x_0} g(x) dx + \\int_{x_0}^{\\infty} g(x) dx\\]\nFor the first part of the integral:\n\\[\\int_{-\\infty}^{x_0} g(x) dx = \\int_{-\\infty}^{x_0} 0 dx = 0\\]\nFor the second part:\n\\[\\int_{x_0}^{\\infty} g(x) dx = \\int_{x_0}^{\\infty} \\frac{f(x)}{1 - F(x_0)} dx\\]\n\\[= \\frac{1}{1 - F(x_0)} \\int_{x_0}^{\\infty} f(x) dx\\]\n\\[= \\frac{1}{1 - F(x_0)} [1 - F(x_0)]\\]\n\\[= 1\\]\nThe last step uses the fact that \\(\\int_{x_0}^{\\infty} f(x) dx = 1 - F(x_0)\\), which is the probability that \\(X &gt; x_0\\).\nTherefore, the total integral is:\n\\[\\int_{-\\infty}^{\\infty} g(x) dx = 0 + 1 = 1\\]\n\n(answer \"We have proven that $g(x)$ satisfies both conditions to be a pdf: it is non-negative for all $x$, and its integral over the entire domain equals 1.\")\n\n\nWe have proven that \\(g(x)\\) satisfies both conditions to be a pdf: it is non-negative for all \\(x\\), and its integral over the entire domain equals 1.\n\nInterpretation:\nThe function \\(g(x)\\) represents the conditional probability density of \\(X\\) given that \\(X \\geq x_0\\). This is why:\n\nIt’s zero for \\(x &lt; x_0\\), as we’re only considering the case where \\(X \\geq x_0\\).\nFor \\(x \\geq x_0\\), it’s the original pdf \\(f(x)\\) scaled up by a factor of \\(1 / (1 - F(x_0))\\). This scaling ensures that the total probability for \\(X \\geq x_0\\) sums to 1.\n\nThis type of function is often used in survival analysis and reliability theory, where we’re interested in the distribution of a variable given that it has survived up to a certain point.\nConclusion:\nWe have proven that \\(g(x)\\) is indeed a valid probability density function. This result is important in probability theory and statistics, particularly when dealing with truncated or conditional distributions. It shows how we can derive a new pdf from an existing one by conditioning on a certain event (in this case, \\(X \\geq x_0\\)). This concept has applications in various fields, including actuarial science, engineering reliability, and medical survival analysis.\n\nsource: src/assignments/hw3/q6.clj",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Question 6</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q7.html",
    "href": "assignments.hw3.q7.html",
    "title": "Question 7",
    "section": "",
    "text": "(ns assignments.hw3.q7\n  (:require\n   [assignments.hw3.utils :refer :all]))\n\n\n\n7) Assume that the AI (artificial intelligence) system of an electric car emits 10 errors (including pattern detection errors due to insufficient training data or operational errors) in a trip of 3,000 miles. Also assume that the errors are randomly distributed to 8 back-up circuits where one circuit may handle more than one errors. If all back-up circuits receive at least one error in one trip, a warning light the control panel automatically turns on. Find the probability that the warning light turns on during a trip of 3000 miles?\nTo solve this problem, we need to find the probability that all 8 back-up circuits receive at least one error out of the 10 errors emitted. This is equivalent to finding the probability of distributing 10 distinct objects (errors) into 8 distinct boxes (circuits) such that no box is empty.\nThis is a classic occupancy problem, and we can solve it using the concept of Stirling numbers of the second kind and the principle of inclusion-exclusion.\nLet’s break down the solution:\n\nTotal number of ways to distribute 10 errors to 8 circuits: \\(8^{10}\\) (Each error has 8 choices, and we make this choice 10 times)\nNumber of ways to distribute 10 errors to 8 circuits with all circuits receiving at least one error:\n\nThis is given by the Stirling number of the second kind: \\(S(10,8)\\)\nWe also need to account for the number of ways to arrange the 8 circuits, which is \\(8!\\)\nTherefore, the number of favorable outcomes is \\(8! * S(10,8)\\)\n\nProbability = (Favorable outcomes) / (Total outcomes) = \\(8! * S(10,8) / 8^{10}\\)\n\n\n(comment\n  (defn factorial [n]\n    (apply * (range 1 (inc n))))\n\n  (defn stirling2 [n k]\n    (cond\n      (or (= k 0) (&gt; k n)) 0\n      (or (= k 1) (= k n)) 1\n      :else (+ (* k (stirling2 (dec n) k))\n               (stirling2 (dec n) (dec k)))))\n\n  (defn probability-all-circuits-receive-error []\n    (let [n 10                                            ; number of errors\n          k 8                                             ; number of circuits\n          total-ways (Math/pow k n)\n          favorable-ways (* (factorial k) (stirling2 n k))]\n      (/ favorable-ways total-ways))))\n\n\n(comment\n  (defn probability-all-circuits-receive-error []\n    (let [n 10                                            ; number of errors\n          k 8                                             ; number of circuits\n          total-ways (Math/pow k n)\n          favorable-ways (stirling2 n k)]\n      (/ favorable-ways total-ways))))\n\n\n(answer (str \"The probability that the warning light turns on (all circuits receive at least one error) is approximately \"\n             (format \"%.6f\" (probability-all-circuits-receive-error))))\n\n\nThe probability that the warning light turns on (all circuits receive at least one error) is approximately 0.028163\n\nInterpretation:\n\nThe probability is relatively low, which makes sense given that we’re trying to distribute 10 errors among 8 circuits with no empty circuits.\nThis low probability suggests that it’s quite unlikely for all circuits to receive an error in a single trip, which is good for the reliability of the system.\nHowever, even a small probability of all circuits receiving an error could be concerning for a critical system like an AI in an electric car.\n\nVerification:\nWe can verify our result using a Monte Carlo simulation:\n\n(comment\n  (defn simulate-error-distribution []\n    (let [circuits (vec (repeat 8 0))]\n      (reduce (fn [acc _]\n                (update acc (rand-int 8) inc))\n              circuits\n              (range 10)))))\n\n\n(comment\n  (defn all-circuits-have-error? [distribution]\n    (every? pos? distribution)))\n\n\n(comment\n  (defn monte-carlo-simulation [num-simulations]\n    (let [successes (atom 0)]\n      (doseq [_ (range num-simulations)]\n        (when (all-circuits-have-error? (simulate-error-distribution))\n          (swap! successes inc)))\n      (double (/ @successes num-simulations)))))\n\n\n(let [num-simulations 1000000\n      simulated-prob (monte-carlo-simulation num-simulations)]\n  (answer\n   (str \"Simulated probability after \" num-simulations \" simulations: \"\n        (format \"%.6f\" simulated-prob))))\n\n\nSimulated probability after 1000000 simulations: 0.028330\n\nConclusion:\nWe have calculated the probability of the warning light turning on in an electric car’s AI system during a 3000-mile trip. This problem demonstrates the application of Stirling numbers and occupancy problems in a real-world scenario involving error distribution in complex systems.\nThe low probability we found suggests that it’s unlikely for all circuits to receive an error in a single trip, which is generally good for system reliability. However, in critical systems like automotive AI, even low-probability events need to be carefully considered.\nThis type of analysis is crucial in system design and reliability engineering, especially for safety-critical applications like autonomous vehicles. It helps engineers understand the likelihood of various failure modes and design appropriate safeguards and redundancies.\n\nsource: src/assignments/hw3/q7.clj",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Question 7</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q8.html",
    "href": "assignments.hw3.q8.html",
    "title": "Question 8",
    "section": "",
    "text": "(ns assignments.hw3.q8\n  (:require\n    [assignments.hw3.utils :refer :all]))\n\n\n\n8) In 2024, Covid symptoms include running nose, cough, sore throat, fever, chills, body aches, and fatigue. An infected person may develop one or more symptoms simultaneously after three days. If the contaminated airborne particles and droplets spread randomly to four visitors sitting in a waiting room, what is the probability that all visitors developed at least one Covid-symptom after three days?\nTo solve this problem, we need to model it as a probability distribution problem. We can think of this as distributing symptoms (or the virus) to the four visitors, where each visitor must receive at least one symptom.\nThis is similar to the occupancy problem we solved in Question 7, but with a twist: we don’t have a fixed number of ‘objects’ (symptoms) to distribute. Instead, we need to consider the probability of each visitor developing at least one symptom independently.\nLet’s approach this step-by-step:\n\nFirst, let’s consider the probability of a single visitor developing at least one symptom.\nThen, we’ll calculate the probability of all four visitors developing at least one symptom.\n\nFor step 1: - There are 7 possible symptoms. - The probability of not developing a specific symptom is the complement of developing it. - To not develop any symptom, a visitor must not develop each of the 7 symptoms.\n\n(def num-symptoms 7)\n\n\n(defn probability-no-symptoms [p]\n      (Math/pow (- 1 p) num-symptoms))\n\n\n(defn probability-at-least-one-symptom [p]\n      (- 1 (probability-no-symptoms p)))\n\nFor step 2: - All four visitors need to independently develop at least one symptom. - This is the same as the probability of one visitor developing a symptom, raised to the power of 4.\n\n(defn probability-all-visitors-symptomatic [p]\n      (Math/pow (probability-at-least-one-symptom p) 4))\n\nNow, let’s calculate this probability for a range of p values, where p is the probability of developing each individual symptom:\n\n(def p-values [0.1 0.2 0.3 0.4 0.5])\n\n\n(def probabilities-all-visitors\n  (map (fn [p] {:p p :probability (probability-all-visitors-symptomatic p)})\n       p-values))\n\n\n(answer \"Probabilities for all visitors developing at least one symptom:\")\n\n\nProbabilities for all visitors developing at least one symptom:\n\n\n(into [] probabilities-all-visitors)\n\n\n[{:p 0.1, :probability 0.07407875407434654}\n {:p 0.2, :probability 0.3900627838302041}\n {:p 0.3, :probability 0.7090879995682614}\n {:p 0.4, :probability 0.8926403161374201}\n {:p 0.5, :probability 0.9691143073141575}]\n\n\n(doseq [p p-values]\n       (println (str \"For p = \" p \", the probability that all visitors\n  develop at least one symptom is approximately \"\n                     (format \"%.4f\" (probability-all-visitors-symptomatic\n                                           p)))))\n\n\nnil\n\nInterpretation:\n\nAs the probability of developing each individual symptom (p) increases, the probability of all visitors developing at least one symptom increases rapidly.\nEven with relatively low probabilities for individual symptoms, the chance of all four visitors showing symptoms can be quite high due to the multiple symptoms and visitors involved.\nThis demonstrates how easily a highly symptomatic disease can spread in a confined space like a waiting room.\n\nSensitivity Analysis:\nLet’s see how the result changes with the number of visitors:\n\n(defn probability-all-visitors-symptomatic-n [p n]\n      (Math/pow (probability-at-least-one-symptom p) n))\n\n\n(def probabilities-by-visitor-count\n  (map (fn [n] {:visitors n :probability (probability-all-visitors-symptomatic-n 0.2 n)})\n       (range 2 7)))\n\n\n(answer \"Probabilities for different numbers of visitors (p = 0.2):\")\n\n\nProbabilities for different numbers of visitors (p = 0.2):\n\n\n(into [] probabilities-by-visitor-count)\n\n\n[{:visitors 2, :probability 0.6245500651110398}\n {:visitors 3, :probability 0.49357242329626505}\n {:visitors 4, :probability 0.3900627838302041}\n {:visitors 5, :probability 0.308260689106696}\n {:visitors 6, :probability 0.24361373703854744}]\n\n\n(doseq [n (range 2 7)]\n       (let [p 0.2]                                         ; Let's fix p at 0.2 for this analysis\n                 (println (str \"For \" n \" visitors and p = \" p \", the probability is approximately \")\n                                       (format \"%.4f\" (probability-all-visitors-symptomatic-n p n)))))\n\n\nnil\n\nConclusion:\nThis problem demonstrates the application of probability theory to epidemiology and public health. We’ve modeled the spread of Covid symptoms in a waiting room scenario, showing how individual probabilities of developing symptoms combine to affect group outcomes.\nKey takeaways: 1. The probability of all visitors developing symptoms increases with both the individual symptom probability and the number of visitors. 2. Even with moderate individual probabilities, the chance of all members of a group showing symptoms can be high, especially in larger groups. 3. This kind of modeling is crucial for understanding disease spread and informing public health policies, especially in confined spaces like waiting rooms, classrooms, or public transportation.\nIn practice, more complex models would be used, accounting for factors like varying symptom probabilities, incubation periods, and individual susceptibilities. However, this simplified model provides valuable insights into the basic principles of disease spread probability.\n\nsource: src/assignments/hw3/q8.clj",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Question 8</span>"
    ]
  },
  {
    "objectID": "assignments.hw3.q7_methods.html",
    "href": "assignments.hw3.q7_methods.html",
    "title": "Q7: Two Methods",
    "section": "",
    "text": "(ns assignments.hw3.q7-methods\n  (:require\n   [assignments.hw3.utils :refer :all]\n   [scicloj.hanamicloth.v1.api :as haclo]\n   [tablecloth.api :as tc]))\n\n\nProblem Statement:\nAssume that the AI system of an electric car emits 9 errors in a trip of 3,000 miles. The errors are randomly distributed to 7 back-up circuits where one circuit may handle more than one error. If all back-up circuits receive at least one error in one trip, a warning light turns on. Find the probability that the warning light turns on during a trip of 3,000 miles.\nWe’ll solve this using two methods:\n\nUsing Stirling numbers of the second kind (combinatorial approach)\nUsing the Inclusion-Exclusion Principle (analytical approach)\n\nLet’s start with a smaller example to illustrate the concepts: 4 errors distributed to 3 circuits.\n\nMethod 1: Using Stirling Numbers of the Second Kind\nThis method uses Stirling numbers of the second kind, \\(S(n,k)\\), which count the ways to partition \\(n\\) distinguishable objects into \\(k\\) non-empty subsets.\nKey function: stirling2\nStirling numbers of the second kind\n\n(comment\n  (defn stirling2 [n k]\n    (cond\n      (or (= k 0) (&gt; k n)) 0\n      (or (= k 1) (= k n)) 1\n      :else (+ (* k (stirling2 (dec n) k))\n               (stirling2 (dec n) (dec k))))))\n\nFor our example (4 errors, 3 circuits):\n\nTotal ways: \\(3^4 = 81\\)\nFavorable ways: \\(3! \\times S(4,3)\\)\nProbability: \\(\\frac{3! \\times S(4,3)}{3^4}\\)\n\nProbability calculation using Method 1\n\n(defn probability-method1 [n k]\n  (let [total-ways (Math/pow k n)\n        favorable-ways (* (factorial k) (stirling2 n k))]\n    (/ favorable-ways total-ways)))\n\nProbability for our example:\n\n(format \"%.4f\" (probability-method1 4 3))\n\n\n\"0.4444\"\n\n\n\nMethod 2: Using the Inclusion-Exclusion Principle\nThis method systematically includes and excludes overlapping cases.\nKey function: inclusion-exclusion-terms\nInclusion-Exclusion terms\n\n(defn inclusion-exclusion-terms-0dex [n k]\n  (map (fn [i]\n         (let [sign (if (even? i) 1 -1)\n               combinations (nck k i)\n               ways (Math/pow (- k i) n)]\n           (* sign combinations ways)))\n       (range 0 (inc k))))\n\n\n(defn inclusion-exclusion-terms-1dex\n  \"Calculates the terms for the inclusion-exclusion principle, corresponding to the formula:\n   \n   n: Number of items to distribute (e.g., calls)\n   k: Number of groups to distribute into (e.g., days)\n   \n   Returns a sequence of terms for the inclusion-exclusion sum.\"\n  [n k]\n  (map (fn [i]\n         (let [sign (if (odd? i) 1 -1)\n               combinations (nck k i)  \n               ways (Math/pow (- k i) n)]  \n           (* sign combinations ways)))\n       (range 1 (inc k))))\n\nFor our example (4 errors, 3 circuits):\nCompute inclusion-exclusion terms for the example\n\n(def terms-example-0dex\n  (map-indexed\n   (fn [i term]\n     {:i i\n      :sign (if (even? i) \"+\" \"-\")\n      :combination (nck 3 i)\n      :ways (Math/pow (- 3 i) 4)\n      :term term})\n   (inclusion-exclusion-terms-0dex 4 3)))\n\n\n(def terms-example-1dex\n  (map-indexed\n   (fn [i term]\n     {:i (inc i)\n      :sign (if (odd? (inc i)) \"+\" \"-\")\n      :combination (nck 3 (inc i))\n      :ways (Math/pow (- 3 (inc i)) 4)\n      :term term})\n   (inclusion-exclusion-terms-1dex 4 3)))\n\nPrint the inclusion-exclusion terms table\n\n(into [] terms-example-0dex)\n\n\n[{:i 0, :sign \"+\", :combination 1N, :ways 81.0, :term 81.0}\n {:i 1, :sign \"-\", :combination 3N, :ways 16.0, :term -48.0}\n {:i 2, :sign \"+\", :combination 3N, :ways 1.0, :term 3.0}\n {:i 3, :sign \"-\", :combination 1N, :ways 0.0, :term -0.0}]\n\n\n(into [] terms-example-1dex)\n\n\n[{:i 1, :sign \"+\", :combination 3N, :ways 16.0, :term 48.0}\n {:i 2, :sign \"-\", :combination 3N, :ways 1.0, :term -3.0}\n {:i 3, :sign \"+\", :combination 1N, :ways 0.0, :term 0.0}]\n\nExplanation:\n\ni: Number of circuits excluded\nsign: Alternates between + and - based on the inclusion-exclusion principle\ncombination: Ways to choose i circuits to exclude\nways: Ways to distribute errors into remaining circuits\nterm: Calculated as sign × combination × ways\n\nThe sum of these terms gives us the favorable ways.\nProbability calculation using Method 2\n\n(defn probability-method2 [n k]\n  (let [total-ways (Math/pow k n)\n        favorable-ways (reduce + (inclusion-exclusion-terms-0dex n k))\n        probs (/ favorable-ways total-ways)]\n    probs))\n\n\n(let [n 4 k 3 \n      total-ways (Math/pow k n)\n      favorable-ways (reduce + (inclusion-exclusion-terms-0dex n k))\n      probs (/ favorable-ways total-ways)]\n  (answer\n   (str \"Probability: \" (format \"%.4f\" probs))))\n\n\nProbability: 0.4444\n\n\n(let [n 4 k 3 \n      total-ways (Math/pow k n)\n      favorable-ways (reduce + (inclusion-exclusion-terms-1dex n k))\n      probs (/ favorable-ways total-ways)\n      not-probs (- 1 probs)]\n  (answer\n   (str \"Probability: \" (format \"%.4f\" not-probs))))\n\n\nProbability: 0.4444\n\n\n\nComparing Methods for Different Values of n\n\n(def n-values (range 1 20))\n\n\n(def k 8)\n\n\n(def probabilities\n  (map (fn [n]\n         {:n n\n          :method1 (double (probability-method1 n k))\n          :method2 (double (probability-method2 n k))})\n       n-values))\n\n\n(def data (tc/dataset probabilities))\n\nPlotting the probabilities\n\n\nConclusion\nBoth methods yield the same results, but they offer different perspectives:\n\nStirling Numbers (Method 1):\n\nFocuses on partitioning errors into non-empty circuits\nMore intuitive for understanding the problem structure\nComputationally efficient for smaller values\n\nInclusion-Exclusion (Method 2):\n\nSystematically accounts for all possible cases\nMore generalizable to complex scenarios\nCan be computationally intensive for large values\n\n\nFor our original problem (10 errors, 8 circuits):\n\n(format \"Probability: %.5f\" (probability-method1 9 7))\n\n\n\"Probability: 0.05770\"\n\nThis low probability suggests it’s unlikely for all circuits to receive an error in a single trip, which is generally good for system reliability. However, in critical systems like automotive AI, even low-probability events need careful consideration.\n\nsource: src/assignments/hw3/q7_methods.clj",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Q7: Two Methods</span>"
    ]
  }
]